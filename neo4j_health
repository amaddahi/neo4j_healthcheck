#!/usr/bin/env python3

# Author: Ali Maddahian
# Last Update 5/13/2019


from datetime import datetime
import pytz
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import sys
import getopt
import time
import os
import dateutil
import re
import datetime
import fnmatch
import subprocess
import string
from matplotlib.backends.backend_pdf import PdfPages
from configobj import ConfigObj

version = '2.0'
verbose = False

db_user = None
db_pwd = None
neo4j_home = None
working_directory = None
metrics_directory = None
results_directory = None
#metrics_csv_interval = 3  # seconds
empty_metrics_directory= False
all_csv_filename = "neo4j_health.csv"
process_metrics = None
dbms_mode = None
bolt_port = 7687
checkdb = None
display_precision=None

def run_benchmark():
    for i in range(1,20):
        cypher="CREATE (EdH:Person {name:'Joe_" + str(i) + "' , born:1950});"
        #print (cypher)
        db(cypher,"my transaction")


def db(cypher,section_title):
       cmd = " echo ' " + cypher + " ' | " + neo4j_home + "/bin/cypher-shell --encryption=false " + \
           " -u " + db_user + " -p " + db_pwd + " --format verbose -a localhost:" + str(bolt_port) + " | egrep -v 'row available|rows available' "
       #print(cmd)
       run_shell(cmd,section_title)

def run_shell(shell_in,section_title):
    file_tmp="/tmp/file.out"
    #print(shell_in)
    shell=shell_in + " > " + file_tmp
    process = subprocess.Popen(shell, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    output = process.communicate()[0]
    exit_code = process.returncode
    
    print ("###################################################################")
    print ("")
    print (section_title)
    print ("")

    if exit_code == 0:
        file = open(file_tmp, 'r') 
        print (file.read() )
        file.close() 
    #else:
        #print("ERROR: " + shell_in)
        #raise error(shell, exit_code, output)
        #raise Exception(shell, output)

    print ("")
    #print ("###################################################################")

def check_size():

    run_shell("df -H -P -l ","Disk Utilization")
    db('call apoc.monitor.store();',"Neo4j Store Size")
    run_shell("grep 'Total size' " + neo4j_home + "/logs/debug.log","Database/Index Size History")
    run_shell("du -h -d1 " + neo4j_home +   "/data/databases/graph.db" ,"Database Size")
    run_shell("du -h -d1 " + neo4j_home +   "/data/databases/graph.db/*tran*" ,"Transaction Logs")


def host_db_check():

    global dbms_mode


    run_shell(
        "echo Uptime: `uptime | awk ' BEGIN { FS = \"up \" } { print $2}' | awk ' BEGIN { FS = \",\" } { print $1 $2 }'` ","")

    db('call apoc.monitor.kernel();',"Neo4j Kernel")

    if neo4j_home is not None:
        run_shell("grep 'Operating System' " + neo4j_home + "/logs/debug.log | head -1","Host Operating System + Number of cpu cores")

    check_size()

    run_shell("top -n 5 -l 1 ","Processes")

    run_shell("exec 2> /dev/null ; vmstat -S k 1 10  ","vmstat")

    run_shell(
        "exec 2> /dev/null ; netstat -an | head -5 ; if [ $? != 0 ] ; then netstat -an | head -5; fi ","netstats")

    #run_shell("exec 2> /dev/null ; ps -ae --forest","Process Tree")

    if neo4j_home is not None:
        run_shell("cd " + neo4j_home + "/logs ; cat debug.log.7 debug.log.6 debug.log.5 debug.log.4 debug.log.3 debug.log.2 debug.log.1 debug.log | grep -i 'o.n.b.i.BackupImpl' " ,"Neo4j Backups")

        run_shell(" grep -i 'INFO' " + neo4j_home + "/logs/query.log | sort -k4nr,4  | cut -c1-280 | head -10  ","Neo4j Top 10 Slowest Queries")

        run_shell(" grep -i 'Detected VM stop-the-world pause' " +
              neo4j_home + "/logs/debug.log | sort -k10 -r | head -10 ","Neo4j Top 10 Longest GC pauses(3.4+)")

        run_shell(" grep -n -i blocked " + neo4j_home +
              "/logs/debug.log | sort -r -n -k 11 | head -10","Neo4j Top 10 Longest GC pauses (< 3.4) ")

        db('call apoc.meta.stats() yield nodeCount as n, relCount as r with  n, r with (2.0*r/((n-1)*n)) as value return round(100 * value)/100;',"Neo4j Node Density ")

        db('call dbms.listQueries();',"Neo4j Active Queries ")

        db('call dbms.listQueries() yield query, elapsedTimeMillis, queryId, username where elapsedTimeMillis > 60000  and NOT query contains toLower(\"LOAD\") with query, collect(queryId)  as q call dbms.killQueries(q) yield queryId return query, queryId;', "Neo4j Active Long Running Queries ( > 60s )")


        run_shell( neo4j_home + "/bin/neo4j-admin memrec ","Recommended Memory Settings")

        run_shell("grep -v '^#\' " + neo4j_home + "/conf/neo4j.conf | sed -e  '/^$/d' | sort ","Non-default Neo4j.conf settings")

    get_config_recommendation()


    if neo4j_home is not None:
        if ((dbms_mode == "CORE") or (dbms_mode == "READ_REPLICA")):
           db('CALL dbms.cluster.role();',"Neo4j Causal Cluster Instance Role")
           db('CALL dbms.cluster.overview();',"Neo4j Causal Cluster Overview")
           db('CALL dbms.cluster.routing.getServers() ;',"Neo4j Causal Cluster Routing")
         

        db('call apoc.monitor.tx();',"Neo4j Transaction")
        db('call apoc.monitor.locks(100);',"Neo4j Locks")

        get_errors()


    print ("###################################################################")

    #print ("")
    #print ("Most recent system state changes")
    #print ("================================")
    #print ("")

def print_dataframe(df):
    global display_precision
    pd.set_option('display.max_rows', 500)
    pd.set_option('display.max_columns', 700)
    pd.set_option('display.width', 1000)
    if display_precision:
       pd.options.display.float_format = '{:,.1f}'.format
    else:
       pd.options.display.float_format = '{:,.0f}'.format
    print(df)
    print('')
    print('')

def get_config_recommendation():





    if neo4j_home is not None:
       config_file = '/tmp/file.out'
       f = open(config_file, "r")
       lines = f.readlines()
       f.close()

       config_on_list=[]
       #config_list=["dbms.memory.pagecache.size","dbms.memory.heap.max_size","dbms.memory.heap.initial_size","causal_clustering.enable_pre_voting","dbms.backup.enabled"]
       config_list=["dbms.memory.pagecache.size","dbms.memory.heap.max_size","dbms.memory.heap.initial_size","causal_clustering.enable_pre_voting"]

       for cfg in config_list:
          for line in lines:
            if cfg in line:
               config_on_list.append(cfg)
               print(config_on_list)

       
       config_off_list = list(set(config_list) - set(list(config_on_list)))

       os.system("grep 'dbms.mode' /tmp/file.out > /tmp/neo4j.tmp")
       config = ConfigObj('/tmp/neo4j.tmp')
       dbms_mode=config.get('dbms.mode') 

       print(' ')
       for item in sorted(config_off_list):
                 if "causal_clustering" not in item:
                      print("[WARNING]:  Please set the following in neo4j.conf: (" + item + ")" )

       if dbms_mode=="CORE":
           if 'causal_clustering.enable_pre_voting' in config_off_list:
                 print("[WARNING]:  Please set the following in neo4j.conf: (causal_clustering.enable_pre_voting)" )
       print(' ')
    return


def get_errors():

        error_keep_list=["Caused by","o.n.c.c.c.RaftMachine", "o.n.c.c.c.s.RaftState","o.n.c.c.c.s.RaftLogShipper","o.n.c.c.c.m.RaftMembershipChanger","o.n.c.c.c.m.RaftMembershipManager","o.n.c.m.RaftOutbound","o.n.k.i.c.MonitorGc","o.n.c.d.HazelcastCoreTopologyService","o.n.c.c.c.m.MembershipWaiterLifecycle","o.n.k.AvailabilityGuard","o.n.i.p.PageCache","o.n.c.i.ClusterBinder","o.n.k.i.DatabaseHealth","o.n.c.c.s.s.CoreStateDownloader","o.n.c.c.IdentityModule","GetOperation","ERROR","WARN","o.n.k.i.f.GraphDatabaseFacadeFactory"]


        print ("###################################################################")
        print ("")
        print ("Interesting Messages in Debug.log")
        print ("")

        debug_file = neo4j_home + "/logs/debug.log"
        f = open(debug_file, "r")
        lines = f.readlines()
        f.close()
   
        debug_filtered_file = neo4j_home + "/logs/filtered_debug.log"
        f = open(debug_filtered_file, "w")

        for line in lines:
            for error in error_keep_list:
                if (error in line) and ('Failed to load') not in line:
                     f.write("----> " + line)
        f.close()

        f = open(debug_filtered_file, "r")
        lines = f.readlines()
        f.close()

        line_count=len(lines) 
        i=0
        for line in lines:
              i=i+1
              if i >= (line_count - 100 ):
                   print(line , end = '')
                     
def get_env():

    global db_user
    global db_pwd
    global neo4j_home
    global working_directory
    global metrics_directory
    global empty_metrics_directory
    global results_directory

    db_user = os.environ.get("DB_USER")
    db_pwd = os.environ.get("DB_PWD")

    working_directory = os.getcwd()
    neo4j_home = os.environ.get('NEO4J_HOME')

    results_directory=working_directory + "/metrics_plots"

    if not os.path.isdir(results_directory):
        os.makedirs(results_directory)

    if neo4j_home:
        dbms_directories_metrics = os.path.join(neo4j_home, 'metrics')

    if os.path.isdir('metrics'):
        if fnmatch.filter(os.listdir('metrics'), '*.csv'):
            metrics_directory = os.path.join(working_directory, 'metrics')
        else:
            empty_metrics_directory=True
            print("")
            print ("Exiting:  Can not find any CSV metrics files in the metrics directory in this folder")
            print("")
            exit(1)
    elif neo4j_home:
        if os.path.isdir(dbms_directories_metrics):
                if fnmatch.filter(os.listdir(dbms_directories_metrics), '*.csv'):
                    metrics_directory = dbms_directories_metrics
                else:
                    empty_metrics_directory=True
                    print("")
                    print("Exiting:  Can not find any CSV metrics files in " + metrics_directory )
                    print("")
                    exit(1)
    else:
        print("")
        print("")
        print       ("Exiting      :  Can not find a metrics directory in this directory nor under $NEO4J_HOME")
        print("")
        if os.getenv("NEO4J_HOME") is None:
              print ("NOTE         :  NEO4J_HOME has not been set!   ")
              print("")
        exit(2)


def get_filenames(metric_category):
    if metric_category == 'server':
        filenames = [
            ["neo4j.server.threads.jetty.all.csv", 0, ""],
            ["neo4j.server.threads.jetty.idle.csv", 0, ""]
        ]
    elif metric_category == 'log_rotation':
        filenames = [
            ["neo4j.log_rotation.events.csv", 0, ""],
            ["neo4j.log_rotation.total_time.csv", 2, ""]
    ]
    elif metric_category == 'causal_clustering':
        filenames = [
            ["neo4j.causal_clustering.catchup.tx_pull_requests_received.csv",1,""],
            ["neo4j.causal_clustering.core.append_index.csv",1,""],
            ["neo4j.causal_clustering.core.commit_index.csv",1,""],
            #["neo4j.causal_clustering.core.dropped_messages.csv",1,""],     # TODO: configurable
            ["neo4j.causal_clustering.core.in_flight_cache.element_count.csv",1,""],
            ["neo4j.causal_clustering.core.in_flight_cache.hits.csv",1,""],
            ["neo4j.causal_clustering.core.in_flight_cache.max_bytes.csv",1,""],
            ["neo4j.causal_clustering.core.in_flight_cache.max_elements.csv",1,""],
            ["neo4j.causal_clustering.core.in_flight_cache.misses.csv",1,""],
            ["neo4j.causal_clustering.core.in_flight_cache.total_bytes.csv",1,""],
            ["neo4j.causal_clustering.core.is_leader.csv",0,""],    # either 0 or 1
            ["neo4j.causal_clustering.core.leader_not_found.csv",1,""],
            ["neo4j.causal_clustering.core.message_processing_delay.csv",0,""],                     # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.append_entries_request.csv",0,""], # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.append_entries_response.csv",0,""],# calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.csv",0,""],                     # CuC or CuT calls/second,milliseconds 
            ["neo4j.causal_clustering.core.message_processing_timer.election_timeout.csv",0,""],
            ["neo4j.causal_clustering.core.message_processing_timer.heartbeat.csv",0,""],           # CuC or CuT calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.heartbeat_response.csv",0,""],  # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.heartbeat_timeout.csv",0,""],   # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.log_compaction_info.csv",0,""], # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.new_batch_request.csv",0,""],   # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.new_entry_request.csv",0,""],   # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.pre_vote_request.csv",0,""],    # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.pre_vote_response.csv",0,""],   # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.prune_request.csv",0,""],       # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.vote_request.csv",0,""],        # calls/second,milliseconds
            ["neo4j.causal_clustering.core.message_processing_timer.vote_response.csv",0,""],       # calls/second,milliseconds
            #["neo4j.causal_clustering.core.queue_sizes.csv",0,""],          #TODO: configurable
            ["neo4j.causal_clustering.core.replication_attempt.csv",1,""],
            ["neo4j.causal_clustering.core.replication_fail.csv",1,""],
            ["neo4j.causal_clustering.core.replication_new.csv",1,""],
            ["neo4j.causal_clustering.core.replication_success.csv",1,""],
            ["neo4j.causal_clustering.core.term.csv",1,""],
            ["neo4j.causal_clustering.core.tx_retries.csv",0,""]
        ]
    elif metric_category == 'network':
        filenames = [
            ["neo4j.network.master_network_store_writes.csv", 0, ""],
            ["neo4j.network.master_network_tx_writes.csv", 0, ""],
            ["neo4j.network.slave_network_tx_writes.csv", 0, ""]
        ]
    elif metric_category == 'object':
        filenames = [
            ["neo4j.ids_in_use.node.csv", 0, ""],
            ["neo4j.ids_in_use.property.csv", 0, ""],
            ["neo4j.ids_in_use.relationship.csv", 0, ""],
            ["neo4j.ids_in_use.relationship_type.csv", 0, ""]
        ]
    elif metric_category == 'transaction':
        filenames = [
            ["neo4j.transaction.peak_concurrent.csv", 0, ""],
            ["neo4j.transaction.started.csv", 1, "-ps"],
            ["neo4j.transaction.active.csv", 0, "", 2],
            ["neo4j.transaction.active_read.csv", 0, ""],
            ["neo4j.transaction.active_write.csv", 0, ""],
            ["neo4j.transaction.committed.csv", 1, "-ps"],
            ["neo4j.transaction.committed_read.csv", 1, "-ps"],
            ["neo4j.transaction.committed_write.csv", 1, "-ps"],
            ["neo4j.transaction.rollbacks.csv", 1, "-ps"],
            ["neo4j.transaction.rollbacks_read.csv", 1, "-ps"],
            ["neo4j.transaction.rollbacks_write.csv", 1, "-ps"],
            ["neo4j.transaction.terminated.csv", 1, "-ps"],
            ["neo4j.transaction.terminated_read.csv", 1, "-ps"],
            ["neo4j.transaction.terminated_write.csv", 1, "-ps"]
        ]
    elif metric_category == 'bolt':
        filenames = [
            ["neo4j.bolt.accumulated_processing_time.csv", 2, ""],
            ["neo4j.bolt.accumulated_queue_time.csv", 2, ""],
            ["neo4j.bolt.messages_done.csv", 1, "-ps"],
            ["neo4j.bolt.messages_received.csv", 1, "-ps"],
            ["neo4j.bolt.messages_failed.csv",1, "-ps"],
            ["neo4j.bolt.messages_started.csv", 1, "-ps"],
            ["neo4j.bolt.connections_closed.csv", 1, "-ps"],
            ["neo4j.bolt.connections_opened.csv", 1, "-ps"],
            ["neo4j.bolt.connections_running.csv", 0, "-ps"],
            ["neo4j.bolt.connections_idle.csv", 0, "-ps"],
            ["neo4j.bolt.sessions_started.csv", 1, "-ps"]
        ]
    elif metric_category == 'bolt2':
        filenames = [
            ["neo4j.bolt.accumulated_queue_time.csv", 2, ""],
            ["neo4j.bolt.messages_done.csv", 1, "-ps"]
        ]
    elif metric_category == 'cypher':
        filenames = [
            ["neo4j.cypher.replan_events.csv", 1, "-ps"],
            ["neo4j.cypher.replan_wait_time.csv", 2, ""]
        ]
    elif metric_category == 'page_cache':
        filenames = [
            ["neo4j.page_cache.hit_ratio.csv", 0, ""],
            ["neo4j.page_cache.page_faults.csv", 1, "-ps"],
            ["neo4j.page_cache.hits.csv", 1, "-ps"],
            ["neo4j.page_cache.flushes.csv", 1, "-ps"],
            ["neo4j.page_cache.eviction_exceptions.csv", 1, "-ps"],
            ["neo4j.page_cache.evictions.csv", 1, "-ps"],
            ["neo4j.page_cache.pins.csv", 1, "-ps"],
            ["neo4j.page_cache.unpins.csv", 1, "-ps"]
        ]
    elif metric_category == 'check_point':
        filenames = [
            # collected at checkpoint times only and thus far less frequently collected than other datapointsk
            ["neo4j.check_point.events.csv", 0, ""],
            ["neo4j.check_point.check_point_duration.csv", 3, ""],
            ["neo4j.check_point.total_time.csv", 2, ""]
        ]
    elif metric_category == 'jvm_thread':
        filenames = [
            ["vm.thread.count.csv", 0, ""],
            ["vm.thread.total.csv", 0, ""]
        ]
    elif metric_category == 'jvm_gc2':
        filenames = [
            ["vm.gc.time.g1_young_generation.csv", 2, ""],
            ["vm.gc.time.g1_old_generation.csv", 2, ""]
       ]
    elif metric_category == 'jvm_gc':
        filenames = [
            ["vm.gc.count.g1_old_generation.csv", 0, ""],
            ["vm.gc.count.g1_young_generation.csv", 1, "-ps"],
            ["vm.gc.time.g1_old_generation.csv", 2, ""],
            ["vm.gc.time.g1_young_generation.csv", 2, ""]
        ]

    elif metric_category == 'jvm_memory':
        filenames = [
            ["vm.memory.buffer.direct.capacity.csv", 0, "-mb"],
            ["vm.memory.buffer.direct.count.csv", 0, ""],
            ["vm.memory.buffer.direct.used.csv", 0, "-mb"],
            ["vm.memory.buffer.mapped.capacity.csv", 0, "-mb"],
            ["vm.memory.buffer.mapped.count.csv", 0, ""],
            ["vm.memory.buffer.mapped.used.csv", 0, "-mb"],
            ["vm.memory.pool.code_cache.csv", 0, "-mb"],
            ["vm.memory.pool.compressed_class_space.csv", 0, "-mb"],
            ["vm.memory.pool.g1_eden_space.csv", 0, "-mb"],
            ["vm.memory.pool.g1_old_gen.csv", 0, "-mb"],
            ["vm.memory.pool.g1_survivor_space.csv", 0, "-mb"],
            ["vm.memory.pool.metaspace.csv", 0, "-mb"]
        ]
    else:
        print ("Exiting:   Incorrect Metric Type.")

    
    filenames_keep=[]
    filenames_delete=[]
    i=0
    for file in filenames:
       metrics_file = metrics_directory + "/" + file[0]
       if not os.path.isfile(metrics_file):
            filenames_delete.append(file)
            #print ("Missing Metric File" + metrics_file)
       else:
            filenames_keep.append(file)
 
       i=i+1

    #print (filenames_keep)
   
    return filenames_keep

def inspect_file_start(metrics_category):
    filenames = get_filenames(metrics_category)

    final_endtime=1735689600
    final_starttime=0

    for file in filenames:
        metrics_file = metrics_directory + "/" + file[0]
        f = open(metrics_file, "r")
        lines = f.readlines()
        f.close()
    
        max_timestamp=0 
        min_timestamp=1735689600 # (1/1/2025)
        
        for line in lines:
            #if ("t," not in line) or ("c" not in line):
            if ("t," not in line):
                cur_timestamp=int(line.split(',')[0])
                if cur_timestamp < min_timestamp:
                    min_timestamp=cur_timestamp
                if cur_timestamp > max_timestamp:
                    max_timestamp=cur_timestamp
                #break

        if max_timestamp < final_endtime:
            final_endtime = max_timestamp
        if min_timestamp > final_starttime:
            final_starttime = min_timestamp

        final_starttime_tz= datetime.datetime.fromtimestamp(final_starttime)
        final_endtime_tz= datetime.datetime.fromtimestamp(final_endtime)

        #print(">>>>>>>>>>>: " + str(final_starttime_tz)  + "    " + str(final_endtime_tz)  + "  " + metrics_file )
        #if (final_starttime >= final_endtime):
               #print("---> detected endtime < starttime: ")

    #print("final_starttime: "  + str(final_starttime)  + "final_endtime: "  + str(final_endtime) )
    #print(" ")

    for file in filenames:
        metrics_file = metrics_directory + "/" + file[0]
        f = open(metrics_file, "r")
        lines = f.readlines()
        f.close()

        metrics_file = metrics_directory + "/" + file[0] + ".final"
        f = open(metrics_file, "w")
        f.write("t,value\n")
        i = 0
        for line in lines:
                i += 1
                if "t," not in line:
                     cur_timestamp=int(line.split(',')[0])
                     #print(line)
                     #print(str(cur_timestamp))
                     #print(str(final_starttime))
                     #print(str(final_endtime))
                     #print(' ')
                     #if (cur_timestamp > final_endtime) or (cur_timestamp < final_starttime):
                if ( final_starttime  < cur_timestamp < final_endtime):
                     f.write(line)
                     #print("--> " + line)
                #if (i == 4):
                     #exit()
        f.close()
     
def flag_server_restarts():
    filenames = get_filenames("server")
    for file in filenames:
        metrics_file = metrics_directory + "/" + file[0]

    f = open(metrics_file, "r")
    lines = f.readlines()
    f.close()

    print ("###################################################################")
    print (" ")
    print ("Database Restarts: ")
    print (" ")

    timestamp_cur=0
    timestamp_prev=0
    flag_start=False
    i = 1
    for line in lines:
        timestamp_prev=timestamp_cur
        if "t," not in line:
            timestamp_cur=int(line.split(',')[0])

        if flag_start is True:
            restart_timestamp=int(line.split(',')[0])
            converted_restart_timestamp=time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(restart_timestamp))
            print(converted_restart_timestamp)
            flag_start=False
        if "t," in line:
            flag_start=True
        i += 1
        #print(str(timestamp_prev) + "  " + str(timestamp_cur) + "  " + str(timestamp_cur - timestamp_prev))
    metrics_csv_interval=timestamp_cur-timestamp_prev
    return metrics_csv_interval


def pre_process_files(start_date, end_date, metric_category):

    df2=False
    filenames = get_filenames(metric_category)

    #
    # Paste multiple csv files into a single file for the same metric category
    #
    (filetmp,single_metric_filetmp) = merge_csv_files(filenames, metric_category)

    #
    # Read the merged file with epoch timestamp into a dataframe
    #
    df = pd.read_csv(filetmp)
    if single_metric_filetmp:
        df2 = pd.read_csv(single_metric_filetmp)

    #df = pd.read_csv(filetmp, parse_dates=['date'], date_parser=lambda epoch: pd.to_datetime(epoch, unit='s'))

    os.system("rm " + filetmp)

    #df.to_csv(metric_category + "_0.csv")

    #
    # Change epoch to UTC timestamp
    #
    df = convert_epoch_to_timestamp(df, 'UTC')
    if single_metric_filetmp:
        df2 = convert_epoch_to_timestamp(df2, 'UTC')

    #df.to_csv(metric_category + "_1.csv")

    #
    # drop any rows with null values - indicating mismatched enddates
    #
    df = df.dropna()
    if single_metric_filetmp:
        df2 = df2.dropna()

    #df.to_csv(metric_category + "_2.csv")

    #if single_metric_filetmp:
       #return df,df2
    #else: 
       #return df

    return df,df2

def filter_date_range(start_date, end_date, df, interval, periods, options):

    #
    # if no options are specified, gather stats for the last 24 hours with available data
    #


    start_date_min = df.date.min()
    end_date_max = df.date.max()


    if options is False:
        start_date = df.date.min()
        end_date = df.date.max()
        start_date = end_date - pd.to_timedelta("1day")

    elif (start_date == None):
        if (end_date == None):
            end_date = df.date.max()

        if interval == 'S':
            delta = str(periods)+"S"
        elif interval == 'Min':
            delta = str(periods)+"min"
        elif interval == 'H':
            delta = str(periods)+"hour"
        elif interval == 'D':
            delta = str(periods)+"D"
        elif interval == 'W':
            delta = str(periods)+"W"
        elif interval == 'Y':
            delta = str(periods)+"Y"
        start_date = pd.Timestamp(end_date) - pd.to_timedelta(delta)

    elif (start_date != None and end_date == None):
        if interval == 'S':
            delta = str(periods)+"S"
        elif interval == 'Min':
            delta = str(periods)+"min"
        elif interval == 'H':
            delta = str(periods)+"hour"
        elif interval == 'D':
            delta = str(periods)+"D"
        elif interval == 'W':
            delta = str(periods)+"W"
        elif interval == 'Y':
            delta = str(periods)+"Y"
        end_date = pd.Timestamp(start_date) + pd.to_timedelta(delta)

    print ('min_startdate:           ', pd.Timestamp(start_date_min))
    print ('max_enddate:             ', pd.Timestamp(end_date_max))
    print ('')
    print ('search_startdate:        ', pd.Timestamp(start_date))
    print ('search_enddate:          ', pd.Timestamp(end_date))
    print ('')
    print ('')

    if ( pd.Timestamp(start_date) > pd.Timestamp(end_date_max) ):
         print(" ")
         print(" ")
         print ("Exiting:  Invalid Start Search Date(" +  str(pd.Timestamp(start_date)) + ") is outside of the Max range of available metrics(" + str(pd.Timestamp(end_date_max)) + ")" )
         print(" ")
         print(" ")
         return 1

    if ( pd.Timestamp(end_date) < pd.Timestamp(start_date_min) ):
         print(" ")
         print(" ")
         print ("Exiting:  Invalid Start Search Date(" +  str(pd.Timestamp(start_date)) + ") is outside of the Max range of available metrics(" + str(pd.Timestamp(end_date_max)) + ")" )
         print(" ")
         print(" ")
         return 1

    if ( pd.Timestamp(end_date) > pd.Timestamp(end_date_max) ):
         df = df[df.date < end_date_max]
    else:
         df = df[df.date < end_date]
 
    if ( pd.Timestamp(start_date) < pd.Timestamp(start_date_min) ):
         df = df[df.date > start_date_min]
    else:
         df = df[df.date > start_date]

    return df


def process_cumulative_timings(interval, df, df_org, metric_category):

    #print(list(df))
    df_org = df_org.dropna()

    filenames = get_filenames(metric_category)

    (metric_category_abbr, db_category_abbr) = get_metric_category_abbr(metric_category)

    for file in filenames:
        x1 = re.sub(db_category_abbr + metric_category +
                    '.', metric_category_abbr, file[0])
        # x2 = re.sub('.csv','', x1)
        val_label = re.sub('.csv', '', x1)
        # val_label = re.sub('metrics/','', x2)
        val_label = val_label + file[2]

        if file[1] == 2:  

            df[val_label] = np.ceil(df[val_label].diff()).fillna(0)

            #if metric_category == 'bolt':
            #df[val_label] = np.ceil(df[val_label].diff()/(df_org["blt.messages_done-ps"].diff())).fillna(0)

            if metric_category == 'jvm_gc':
                df["Total_GC_pause_time-ms"]= np.ceil(df['vm.gc.time.g1_young_generation'] +  df['vm.gc.time.g1_old_generation']).fillna(0).astype(int)

            # Set negative diffs (due to restarts) to zero
            #df[val_label][df[val_label] < 0] = 0
            df[val_label][df[val_label] < 0] = 0

    df = df.dropna()
    #print(df[val_label] - df_org[val_label])
    return df


def process_cumulative_counters(interval, df, metric_category):

    filenames = get_filenames(metric_category)

    (metric_category_abbr, db_category_abbr) = get_metric_category_abbr(metric_category)

    for file in filenames:
        x1 = re.sub(db_category_abbr + metric_category +
                    '.', metric_category_abbr, file[0])
        # x2 = re.sub('.csv','', x1)
        val_label = re.sub('.csv', '', x1)
        # val_label = re.sub('metrics/','', x2)
        val_label = val_label + file[2]

        # this is to address the warning when using df[val_label][df[val_label] < 0] = 0
        pd.set_option('mode.chained_assignment', None)

        #add a counter for total 
        if file[1] == 1:  # obtain rate of change per second between two points in time

            #if( metric_category == 'bolt' ):
                   #df[val_label] = df[val_label].diff().fillna(0)
            #else:

                   df[val_label] = (df[val_label].diff()).fillna(0)
                   df[val_label][df[val_label] < 0] = 0

                   #df[val_label] = (df[val_label].diff()/metrics_csv_interval).fillna(0)
                   #df[val_label] = np.ceil(df[val_label].diff()/metrics_csv_interval).fillna(0).astype(int)
                   #df[val_label] = np.ceil(df[val_label].diff()).fillna(0).astype(int)

        elif ((file[1] == 0) and ( metric_category == 'jvm_memory' )):
            df["heap_size-mb"]= df['vm.memory.pool.code_cache-mb'] +  		\
                             df['vm.memory.pool.compressed_class_space-mb'] + 	\
                             df['vm.memory.pool.g1_eden_space-mb'] + 		\
                             df['vm.memory.pool.g1_old_gen-mb'] + 			\
                             df['vm.memory.pool.metaspace-mb']/(1024*1024)

            df[val_label]=df[val_label]/(1024*1024)
            df[val_label][df[val_label] < 0] = 0
         
    return df

def normalize_stats(df, metric_category):

    if (metric_category == 'bolt') :
                df["blt.accumulated_processing_time_Avg"] = (df["blt.accumulated_processing_time_Avg"]/(df["blt.messages_done-ps_Avg"])).fillna(0)
                df["blt.accumulated_processing_time_Max"] = (df["blt.accumulated_processing_time_Max"]/(df["blt.messages_done-ps_Avg"])).fillna(0)
                df["blt.accumulated_queue_time_Avg"] = (df["blt.accumulated_queue_time_Avg"]/(df["blt.messages_done-ps_Avg"])).fillna(0)
                df["blt.accumulated_queue_time_Max"] = (df["blt.accumulated_queue_time_Max"]/(df["blt.messages_done-ps_Avg"])).fillna(0)
    elif (metric_category == 'cypher') :
                # TODO
                df["cyp.replan_events-ps_Sum"] = (df["cyp.replan_events-ps_Sum"]*metrics_csv_interval).fillna(0)
                df["cyp.replan_events-ps_Avg"] = (df["cyp.replan_events-ps_Avg"]*metrics_csv_interval).fillna(0)
                df["cyp.replan_events-ps_Max"] = (df["cyp.replan_events-ps_Max"]*metrics_csv_interval).fillna(0)
                df["cyp.replan_wait_time_Avg"] = (df["cyp.replan_wait_time_Avg"]/(df["cyp.replan_events-ps_Avg"])).fillna(0)
                df["cyp.replan_wait_time_Max"] = (df["cyp.replan_wait_time_Max"]/(df["cyp.replan_events-ps_Avg"])).fillna(0)

    return df

def apply_output_projection(df,metric_category):

        column_names = list(df)
        #print (column_names)

        if metric_category == 'check_point':
            output_keep_list = ['ckp.events_Count', 'ckp.total_time_Sum', 'ckp.check_point_duration_Sum', 'ckp.check_point_duration_Count', 'ckp.check_point_duration_Avg', 'ckp.check_point_duration_Max']
        elif metric_category == 'causal_clustering':
            output_keep_list = ['cc.catchup.tx_pull_requests_received_Sum', 'cc.catchup.tx_pull_requests_received_Avg', 'cc.catchup.tx_pull_requests_received_Max' , 'cc.core.append_index_Sum', 'cc.core.append_index_Avg', 'cc.core.append_index_Max', 'cc.core.commit_index_Sum', 'cc.core.commit_index_Avg', 'cc.core.commit_index_Max', 'cc.core.in_flight_cache.element_count_Sum', 'cc.core.in_flight_cache.element_count_Avg', 'cc.core.in_flight_cache.element_count_Max', 'cc.core.in_flight_cache.hits_Sum', 'cc.core.in_flight_cache.hits_Avg', 'cc.core.in_flight_cache.hits_Max', 'cc.core.in_flight_cache.max_bytes_Sum', 'cc.core.in_flight_cache.max_bytes_Avg', 'cc.core.in_flight_cache.max_bytes_Max', 'cc.core.in_flight_cache.max_elements_Sum', 'cc.core.in_flight_cache.max_elements_Avg', 'cc.core.in_flight_cache.max_elements_Max' , 'cc.core.in_flight_cache.misses_Sum', 'cc.core.in_flight_cache.misses_Avg', 'cc.core.in_flight_cache.misses_Max', 'cc.core.in_flight_cache.total_bytes_Sum', 'cc.core.in_flight_cache.total_bytes_Avg', 'cc.core.in_flight_cache.total_bytes_Max', 'cc.core.is_leader_Sum', 'cc.core.is_leader_Avg', 'cc.core.is_leader_Max', 'cc.core.leader_not_found_Sum', 'cc.core.leader_not_found_Avg', 'cc.core.leader_not_found_Max', 'cc.core.message_processing_delay_Sum', 'cc.core.message_processing_delay_Avg', 'cc.core.message_processing_delay_Max', 'cc.core.message_processing_timer.append_entries_request_Sum', 'cc.core.message_processing_timer.append_entries_request_Avg', 'cc.core.message_processing_timer.append_entries_request_Max', 'cc.core.message_processing_timer.append_entries_response_Sum' , 'cc.core.message_processing_timer.append_entries_response_Avg', 'cc.core.message_processing_timer.append_entries_response_Max', 'cc.core.message_processing_timer_Sum'  , 'cc.core.message_processing_timer_Avg', 'cc.core.message_processing_timer_Max', 'cc.core.message_processing_timer.election_timeout_Sum', 'cc.core.message_processing_timer.election_timeout_Avg', 'cc.core.message_processing_timer.election_timeout_Max', 'cc.core.message_processing_timer.heartbeat_Sum', 'cc.core.message_processing_timer.heartbeat_Avg', 'cc.core.message_processing_timer.heartbeat_Max', 'cc.core.message_processing_timer.heartbeat_response_Sum', 'cc.core.message_processing_timer.heartbeat_response_Avg', 'cc.core.message_processing_timer.heartbeat_response_Max', 'cc.core.message_processing_timer.heartbeat_timeout_Sum', 'cc.core.message_processing_timer.heartbeat_timeout_Avg', 'cc.core.message_processing_timer.heartbeat_timeout_Max', 'cc.core.message_processing_timer.log_compaction_info_Sum', 'cc.core.message_processing_timer.log_compaction_info_Avg', 'cc.core.message_processing_timer.log_compaction_info_Max', 'cc.core.message_processing_timer.new_batch_request_Sum', 'cc.core.message_processing_timer.new_batch_request_Avg', 'cc.core.message_processing_timer.new_batch_request_Max', 'cc.core.message_processing_timer.new_entry_request_Sum', 'cc.core.message_processing_timer.new_entry_request_Avg', 'cc.core.message_processing_timer.new_entry_request_Max', 'cc.core.message_processing_timer.pre_vote_request_Sum', 'cc.core.message_processing_timer.pre_vote_request_Avg', 'cc.core.message_processing_timer.pre_vote_request_Max', 'cc.core.message_processing_timer.pre_vote_response_Sum', 'cc.core.message_processing_timer.pre_vote_response_Avg', 'cc.core.message_processing_timer.pre_vote_response_Max', 'cc.core.message_processing_timer.prune_request_Sum', 'cc.core.message_processing_timer.prune_request_Avg', 'cc.core.message_processing_timer.prune_request_Max', 'cc.core.message_processing_timer.vote_request_Sum', 'cc.core.message_processing_timer.vote_request_Avg', 'cc.core.message_processing_timer.vote_request_Max', 'cc.core.message_processing_timer.vote_response_Sum', 'cc.core.message_processing_timer.vote_response_Avg', 'cc.core.message_processing_timer.vote_response_Max', 'cc.core.replication_attempt_Sum', 'cc.core.replication_attempt_Avg', 'cc.core.replication_attempt_Max', 'cc.core.replication_fail_Sum', 'cc.core.replication_fail_Avg', 'cc.core.replication_fail_Max', 'cc.core.replication_new_Sum', 'cc.core.replication_new_Avg', 'cc.core.replication_new_Max', 'cc.core.replication_success_Sum', 'cc.core.replication_success_Avg', 'cc.core.replication_success_Max', 'cc.core.term_Sum', 'cc.core.term_Avg', 'cc.core.term_Max', 'cc.core.tx_retries_Sum', 'cc.core.tx_retries_Avg', 'cc.core.tx_retries_Max']
        elif metric_category == 'cypher':
            output_keep_list = ['cyp.replan_events-ps_Sum', 'cyp.replan_events-ps_Avg', 'cyp.replan_events-ps_Max', 'cyp.replan_wait_time_Sum', 'cyp.replan_wait_time_Avg', 'cyp.replan_wait_time_Max']
        elif metric_category == 'server':
            output_keep_list = ['srv.threads.jetty.all_Avg', 'srv.threads.jetty.all_Max', 'srv.threads.jetty.idle_Avg', 'srv.threads.jetty.idle_Max'] 
        elif metric_category == 'object':
            output_keep_list = ['neo4j.ids_in_use.node_Max', 'neo4j.ids_in_use.property_Max', 'neo4j.ids_in_use.relationship_Max', 'neo4j.ids_in_use.relationship_type_Max']
        elif metric_category == 'log_rotation':
            output_keep_list = ['log.events_Sum', 'log.events_Avg', 'log.events_Max', 'log.total_time_Sum', 'log.total_time_Avg', 'log.total_time_Max']
        elif metric_category == 'page_cache':
            output_keep_list = ['pgc.hit_ratio_Avg', 'pgc.hit_ratio_Max', 'pgc.page_faults-ps_Sum', 'pgc.page_faults-ps_Avg', 'pgc.page_faults-ps_Max', 'pgc.hits-ps_Sum', 'pgc.hits-ps_Avg', 'pgc.hits-ps_Max', 'pgc.flushes-ps_Sum', 'pgc.flushes-ps_Avg', 'pgc.flushes-ps_Max', 'pgc.eviction_exceptions-ps_Sum', 'pgc.eviction_exceptions-ps_Avg', 'pgc.eviction_exceptions-ps_Max', 'pgc.evictions-ps_Sum', 'pgc.evictions-ps_Avg', 'pgc.evictions-ps_Max', 'pgc.pins-ps_Sum', 'pgc.pins-ps_Avg', 'pgc.pins-ps_Max', 'pgc.unpins-ps_Sum', 'pgc.unpins-ps_Avg', 'pgc.unpins-ps_Max']
        elif metric_category == 'jvm_gc':
            output_keep_list = ['vm.gc.count.g1_old_generation_Sum',  'vm.gc.count.g1_old_generation_Avg', 'vm.gc.count.g1_old_generation_Max', 'vm.gc.count.g1_young_generation-ps_Sum', 'vm.gc.count.g1_young_generation-ps_Avg', 'vm.gc.count.g1_young_generation-ps_Max', 'vm.gc.time.g1_old_generation_Sum', 'vm.gc.time.g1_old_generation_Avg', 'vm.gc.time.g1_old_generation_Max', 'vm.gc.time.g1_young_generation_Sum', 'vm.gc.time.g1_young_generation_Avg', 'vm.gc.time.g1_young_generation_Max', 'Total_GC_pause_time-ms_Sum', 'Total_GC_pause_time-ms_Avg', 'Total_GC_pause_time-ms_Max']
        elif metric_category == 'jvm_thread':
            output_keep_list = ['vm.thread.count_Sum', 'vm.thread.count_Avg', 'vm.thread.count_Max', 'vm.thread.total_Sum', 'vm.thread.total_Avg', 'vm.thread.total_Max']
        elif metric_category == 'jvm_memory':
            output_keep_list = ['vm.memory.buffer.direct.capacity-mb_Sum', 'vm.memory.buffer.direct.capacity-mb_Avg', 'vm.memory.buffer.direct.capacity-mb_Max', 'vm.memory.buffer.direct.count_Sum', 'vm.memory.buffer.direct.count_Avg', 'vm.memory.buffer.direct.count_Max', 'vm.memory.buffer.direct.used-mb_Sum', 'vm.memory.buffer.direct.used-mb_Avg', 'vm.memory.buffer.direct.used-mb_Max', 'vm.memory.buffer.mapped.capacity-mb_Sum', 'vm.memory.buffer.mapped.capacity-mb_Avg', 'vm.memory.buffer.mapped.capacity-mb_Max', 'vm.memory.buffer.mapped.count_Sum', 'vm.memory.buffer.mapped.count_Avg', 'vm.memory.buffer.mapped.count_Max', 'vm.memory.buffer.mapped.used-mb_Sum', 'vm.memory.buffer.mapped.used-mb_Avg', 'vm.memory.buffer.mapped.used-mb_Max', 'vm.memory.pool.code_cache-mb_Sum', 'vm.memory.pool.code_cache-mb_Avg', 'vm.memory.pool.code_cache-mb_Max', 'vm.memory.pool.compressed_class_space-mb_Sum', 'vm.memory.pool.compressed_class_space-mb_Avg', 'vm.memory.pool.compressed_class_space-mb_Max', 'vm.memory.pool.g1_eden_space-mb_Sum', 'vm.memory.pool.g1_eden_space-mb_Avg', 'vm.memory.pool.g1_eden_space-mb_Max', 'vm.memory.pool.g1_old_gen-mb_Sum', 'vm.memory.pool.g1_old_gen-mb_Avg', 'vm.memory.pool.g1_old_gen-mb_Max', 'vm.memory.pool.g1_survivor_space-mb_Sum', 'vm.memory.pool.g1_survivor_space-mb_Avg', 'vm.memory.pool.g1_survivor_space-mb_Max', 'vm.memory.pool.metaspace-mb_Sum', 'vm.memory.pool.metaspace-mb_Avg', 'vm.memory.pool.metaspace-mb_Max', 'heap_size-mb_Sum', 'heap_size-mb_Avg', 'heap_size-mb_Max']
        elif metric_category == 'transaction':
            output_keep_list = ['tx.peak_concurrent_Avg', 'tx.peak_concurrent_Max', 
                                 'tx.started-ps_Sum', 'tx.started-ps_Avg', 'tx.started-ps_Max', 
				 'tx.active_Sum', 'tx.active_Avg', 'tx.active_Max', 'tx.active_read_Sum', 'tx.active_read_Avg', 'tx.active_read_Max', 'tx.active_write_Sum', 'tx.active_write_Avg', 'tx.active_write_Max', 'tx.committed-ps_Sum', 'tx.committed-ps_Avg', 'tx.committed-ps_Max', 'tx.committed_read-ps_Sum', 'tx.committed_read-ps_Avg', 'tx.committed_read-ps_Max', 'tx.committed_write-ps_Sum', 'tx.committed_write-ps_Avg', 'tx.committed_write-ps_Max', 'tx.rollbacks-ps_Sum', 'tx.rollbacks-pshgCount', 'tx.rollbacks-ps_Avg', 'tx.rollbacks-ps_Max', 'tx.rollbacks_read-ps_Sum', 'tx.rollbacks_read-ps_Avg', 'tx.rollbacks_read-ps_Max', 'tx.rollbacks_write-ps_Sum', 'tx.rollbacks_write-ps_Avg', 'tx.rollbacks_write-ps_Max', 'tx.terminated-ps_Sum', 'tx.terminated-ps_Avg', 'tx.terminated-ps_Max', 'tx.terminated_read-ps_Sum', 'tx.terminated_read-ps_Avg', 'tx.terminated_read-ps_Max', 'tx.terminated_write-ps_Sum', 'tx.terminated_write-ps_Avg', 'tx.terminated_write-ps_Max']
        elif metric_category == 'bolt':
            output_keep_list = ['blt.accumulated_processing_time_Sum', 'blt.accumulated_processing_time_Avg', 'blt.accumulated_processing_time_Max', 'blt.accumulated_queue_time_Sum',  'blt.accumulated_queue_time_Avg', 'blt.accumulated_queue_time_Max', 'blt.messages_done-ps_Sum', 'blt.messages_done-ps_Avg', 'blt.messages_done-ps_Max', 'blt.messages_received-ps_Sum', 'blt.messages_received-ps_Avg', 'blt.messages_received-ps_Max', 'blt.messages_failed-ps_Sum', 'blt.messages_failed-ps_Avg', 'blt.messages_failed-ps_Max', 'blt.messages_started-ps_Sum', 'blt.messages_started-ps_Avg', 'blt.messages_started-ps_Max', 'blt.connections_closed-ps_Sum', 'blt.connections_closed-ps_Avg', 'blt.connections_closed-ps_Max', 'blt.connections_opened-ps_Sum', 'blt.connections_opened-ps_Avg', 'blt.connections_opened-ps_Max', 'blt.connections_running-ps_Avg', 'blt.connections_running-ps_Max', 'blt.connections_idle-ps_Avg', 'blt.connections_idle-ps_Max', 'blt.sessions_started-ps_Sum', 'blt.sessions_started-ps_Avg', 'blt.sessions_started-ps_Max']

        output_delete_list = list(set(column_names) - set(output_keep_list))
        df_output = df.drop(output_delete_list, 1, errors='ignore')
        #print(df_output)
     
        for label in list(df_output):
            #print (label)  
            pattern_sum='-ps_Sum'
            pattern_count='-ps_Count'
            pattern_avg='-ps_Avg'
            pattern_max='-ps_Max'
            pattern_cc_message='core.message_processing'
            pattern_cc_core='cc.core'
            if pattern_sum in label:
                df_output.rename(columns={label:label.replace(pattern_sum,'_Sum')}, inplace=True)
                #print (label + "   " + label.replace(pattern_sum,'_Sum'))
            elif pattern_count in label:
                df_output.rename(columns={label:label.replace(pattern_count,'_Count')}, inplace=True)
                #print (label + "   " + label.replace(pattern_count,'_Count'))
            elif ((pattern_avg in label) and (metric_category == 'bolt') and (('connections_running' in label) or ('connections_idle' in label))) :
                df_output.rename(columns={label:label.replace(pattern_avg,'_Avg')}, inplace=True)
                #print (label + "   " + label.replace(pattern_avg,'_Avg'))
            elif ((pattern_max in label) and (metric_category == 'bolt') and (('connections_running' in label) or ('connections_idle' in label))) :
                df_output.rename(columns={label:label.replace(pattern_max,'_Max')}, inplace=True)
            elif ((pattern_max in label) and (metric_category == 'cypher') and ('event' in label) ) :
                df_output.rename(columns={label:label.replace(pattern_max,'_Max')}, inplace=True)
            elif ((pattern_avg in label) and (metric_category == 'cypher') and ('event' in label) ) :
                df_output.rename(columns={label:label.replace(pattern_avg,'_Avg')}, inplace=True)
            elif (metric_category == 'causal_clustering')  :
                if (pattern_cc_message in label):
                     df_output.rename(columns={label:label.replace(pattern_cc_message,'msg_proc')}, inplace=True)
                elif (pattern_cc_core in label) :
                     df_output.rename(columns={label:label.replace(pattern_cc_core,'cc')}, inplace=True)
            
            #print (label + "   " + label.replace(pattern_max,'_Max'))
                
        return df_output


def get_stats(start_date, end_date, metric_category, interval, periods, options):

    print (" ")
    print ("###################################################################")
    print (" ")
    msg= "Summarizing Stats:  " + metric_category

    if metric_category == 'network':
       for file in os.listdir(metrics_directory):
           if fnmatch.fnmatch(file, "*causal_clustering*"):
               msg = msg + "  --> Collected for HA clusters only"
               print(msg)
               return
    else:
       print(msg)
           
    print (" ")
    print (" ")
    (df,df2) = pre_process_files(start_date, end_date, metric_category)

    #df.to_csv(metric_category + "1.csv")
    #df2.to_csv(metric_category + "12.csv")

    # can this be moved to main?
    #
    if start_date == None and end_date == None:
        aggr_interval = "H"

    df = filter_date_range(start_date, end_date, df, interval, periods, options)
    if isinstance(df2 , pd.DataFrame):
        df2 = filter_date_range(start_date, end_date, df2, interval, periods, options)

    if not isinstance(df, pd.DataFrame):
        if df==1:
            return 

    df_org=df

    #df.to_csv(metric_category + "filtered_date_2.csv")
    # print_dataframe(df.tail(5))

    df = process_cumulative_counters(interval, df, metric_category)

    #df.to_csv(metric_category + "_cumu_cntr_3.csv")

    df = process_cumulative_timings(interval, df, df_org, metric_category)

    #df.to_csv(metric_category + "cumu_time_4.csv")

    if options is False:
        interval = "H"

    df = df.replace([np.inf, -np.inf], np.nan)

    #print_dataframe(df.head(5))
    # print(df.index.tolist())

    #df = np.ceil(df.groupby(pd.Grouper(key='date', freq=interval)).agg( [('Sum', 'sum') , ('Count', 'count'), ('Avg', 'mean'), ('Max', 'max')] )).fillna(0).astype(int)
    df = (df.groupby(pd.Grouper(key='date', freq=interval)).agg( [('Sum', 'sum') , ('Count', 'count'), ('Avg', 'mean'), ('Max', 'max')] )).fillna(0)

    #print_dataframe(df.tail(15).T)
    if isinstance(df2 , pd.DataFrame):
          #df2 = np.ceil(df2.groupby(pd.Grouper(key='date', freq=interval)).agg( [('Sum', 'sum') , ('Count', 'count'), ('Avg', 'mean'), ('Max', 'max')] )).fillna(0).astype(int)
          df2 = (df2.groupby(pd.Grouper(key='date', freq=interval)).agg( [('Sum', 'sum') , ('Count', 'count'), ('Avg', 'mean'), ('Max', 'max')] )).fillna(0)
          #df2 = np.ceil(df2.groupby(pd.Grouper(key='date', freq=interval)).agg( [('Avg', 'mean'), ('Max', 'max')] )).fillna(0).astype(int)

    
    #df.to_csv(metric_category + "avg-max-5.csv")

    # Reformat dataframe column labels
    #
    df.columns = ["_".join(x) for x in df.columns.ravel()]
    if isinstance(df2 , pd.DataFrame):
         df2.columns = ["_".join(x) for x in df2.columns.ravel()]

    #print(list(df))

    df = normalize_stats(df,metric_category)

    # When aggregating by the seconds, remove dataframe columns where all different metrics are zero  
    #
    if interval in ('S','Min'):
       df=df[(df.sum(axis=1) != 0)]       

    # if check_point then df = df_results

    if isinstance(df2 , pd.DataFrame):
        df = df.merge(df2,how='left', left_on='date', right_on='date')


    df = apply_output_projection(df,metric_category)

    print_dataframe(df.tail(15).T)

    # TODO - check this
    #
    if process_metrics == 'all':
        csv_filename = results_directory + "/" + all_csv_filename
        df.to_csv(csv_filename,mode='a')
    else:
        csv_filename = results_directory + "/" + metric_category + ".csv"
        df.to_csv(csv_filename)

    plot_metric(df, metric_category)


def get_plot_group(metric_category):
    if metric_category == 'transaction':
        plot_group = [
            ["peak_concurrent"],
            ["active", "started"],
            ["committed", "rollbacks"],
            ["terminated"]
        ]
    elif metric_category == 'log_rotation':
        plot_group = [
            ["rotation"]
        ]
    elif metric_category == 'server':
        plot_group = [
            ["jetty"]
        ]
    elif metric_category == 'object':
        plot_group = [
            ["Max"]
        ]
    elif metric_category == 'network':
        plot_group = [
            ["network"]
        ]
    elif metric_category == 'cypher':
        plot_group = [
            ["replan"]
        ]
    elif metric_category == 'bolt':
        plot_group = [
            ["accumulated"],
            ["messages", "sessions"],
            ["connection"]
        ]
    elif metric_category == 'page_cache':
        plot_group = [
            ["hit_ratio"],
            ["hits", "flushes"],
            ["eviction"],
            ["pin"]
        ]
    elif metric_category == 'check_point':
        plot_group = [
            ["duration"],
            ["events"],
            ["total_time"]
        ]
    elif metric_category == 'jvm_thread':
        plot_group = [
            ["thread"]
        ]
    elif metric_category == 'jvm_gc':
        plot_group = [
            ["count"],
            ["time"]
        ]
    elif metric_category == 'jvm_thread':
        plot_group = [
            ["direct"]
        ]
    elif metric_category == 'jvm_memory':
        plot_group = [
            ["direct"],
            ["mapped"],
            ["code-cache"],
            ["compressed_class_space"],
            ["g1"],
            ["metaspace"]
            
        ]
    elif metric_category == 'causal_clustering':
        plot_group = [
            ["election_timeout"],
            ["pre_vote"],
            ["vote_request","vote_response"],
            ["heartbeat"],
            ["catchup"],
            ["prune_request"],
            ["tx_retries"],
            ["message_processing_delay"],
            ["append_entries_request","append_entries_response"],
            ["log_compaction_info"],
            ["new_batch_request","new_entry_request"],
            ["term"],
            ["is_leader"],
            ["leader_not_found"],
            ["element_count"],
            ["hits","misses"],
            ["max_bytes"],
            ["max_elements"],
            ["total_bytes"],
            ["replication_success","replication_fail"],
            ["replication_new","replication_attempt"],
            ["index"]
        ]

    return plot_group


def check_min_max(df):
    ok_to_print=None 
    if len(df.index) == 1:
        print(' ')
        print ('WARNING: Not enough Data to plot (date.min=date.max), please wait for more data to be captured! ')
        print(' ')
        ok_to_print=False
    else:
        ok_to_print=True
    return ok_to_print


def plot_metric(df, metric_category):

    # check if we have enough data points for a plot  ( need >= 2) 
    #
    if check_min_max(df):
        filenames = get_filenames(metric_category)
        column_names = list(df)
        plot_group = get_plot_group(metric_category)

        #pdf_filename = metric_category + ".pdf"
        pdf_filename = results_directory + "/" + metric_category + ".pdf"

        #with PdfPages(metric_category + ".pdf") as pdf:

        with PdfPages(pdf_filename) as pdf:

            for plot_x in plot_group:
                # print ("GRAPH: " + str(i))
                # i=1
                plot_keep_list = []
                for item in plot_x:
                    for col in column_names:
                        if item in col:
                            plot_keep_list.append(col)

                # Skip the plotting for this metric - means no input file was found earlier
                #
                if not plot_keep_list:
                   continue

                plot_delete_list = list(set(column_names) - set(plot_keep_list))

                df_plot = df.drop(plot_delete_list, 1, errors='ignore')

                df_plot.plot()
                pdf.savefig()
                plt.close()


def convert_date_range_to_seconds(date_range):
    if date_range == "W":
        seconds = 7*24*3600
    elif date_range == "D":
        seconds = 24*3600
    elif date_range == "H":
        seconds = 3600
    return seconds


def convert_epoch_to_timestamp(df, timezone):
    if timezone == "UTC":
        df['date'] = pd.to_datetime(df['date'], unit='s')
    return df


def get_metric_category_abbr(file_type):

    if file_type == "transaction":
        metric_category_abbr = "tx."
        db_category_abbr = "neo4j."
    elif file_type == "page_cache":
        metric_category_abbr = "pgc."
        db_category_abbr = "neo4j."
    elif file_type == "bolt":
        metric_category_abbr = "blt."
        db_category_abbr = "neo4j."
    elif file_type == "causal_clustering":
        metric_category_abbr = "cc."
        db_category_abbr = "neo4j."
    elif file_type == "cypher":
        metric_category_abbr = "cyp."
        db_category_abbr = "neo4j."
    elif file_type == "check_point":
        metric_category_abbr = "ckp."
        db_category_abbr = "neo4j."
    elif file_type == "object":
        metric_category_abbr = "ids."
        db_category_abbr = "neo4j."
    elif file_type == "network":
        metric_category_abbr = "net."
        db_category_abbr = "neo4j."
    elif file_type == "server":
        metric_category_abbr = "srv."
        db_category_abbr = "neo4j."
    elif file_type == "log_rotation":
        metric_category_abbr = "log."
        db_category_abbr = "neo4j."
    elif file_type == "jvm_gc":
        metric_category_abbr = "gc."
        db_category_abbr = "vm."
    elif file_type == "jvm_memory":
        metric_category_abbr = "mem."
        db_category_abbr = "vm."
    elif file_type == "jvm_thread":
        metric_category_abbr = "thr."
        db_category_abbr = "vm."

    return metric_category_abbr, db_category_abbr


def merge_csv_files(filenames, metric_category):
    single_metric_filetmp=False
    all_metric_filetmp = metric_category + ".all_metrics_epoch"
    i = 1

    inspect_file_start(metric_category)
    #exit(0)

    for file in filenames:
       

 
        filetmp_x = working_directory + "/filetmp_x"
        filetmp_i = working_directory + "/filetmp_i"
        metrics_file = metrics_directory + "/" + file[0] + ".final"
        metrics_concat_file = working_directory + "/" + file[0] + ".cat"


        file_type = file[0].split('.')[1]

        if file_type == "thread":
            file_type = "jvm_thread"
        elif file_type == "memory":
            file_type = "jvm_memory"
        elif file_type == "gc":
            file_type = "jvm_memory"
        elif file_type == "ids_in_use":
            file_type = "object"

        (metric_category_abbr, db_category_abbr) = get_metric_category_abbr(file_type)

        #
        # Rename column names
        #
        # string1 = "neo4j." + file_type + "."
        string1 = db_category_abbr + file_type + "."
        val_label = re.sub(
            '', '', (re.sub('.csv', '', (re.sub(string1, metric_category_abbr, file[0])))))

        # Add per second (-ps) to the column names if needed
        val_label = val_label + file[2]
        # print("column:" + val_label)

        # rename the label from value to custom label name
        #
        file_ascii=file[0] + ".ascii"

        # remove any binary non-printable data (might show up as result of transfering files between different host systems
        #
        cmd="sed $'s/[^[:print:]\t]//g'  " + metrics_file  + " > " + file_ascii
        os.system(cmd)
        #cmd="(head -1 " + metrics_file  + " | sed $'s/[^[:print:]\t]//g'  | sed 's/^t/date/g'  && cat `ls -1r " +  metrics_file + "* ` | grep -v '^t'  ) > filename1 && mv filename1 " + metrics_concat_file
        cmd="(head -1 " + file_ascii  + " | sed 's/^t/date/g'  && cat `ls -1r " +  file_ascii + "* ` | grep -v '^t'  ) > filename1 && mv filename1 " + metrics_concat_file
        os.system(cmd)

        cmd = "cut -d, -f1,2 " + metrics_concat_file + " | sed 's/count/value/g' |  sed 's/value/" + val_label + "/g'  > " + metrics_file
        os.system(cmd)


        if file[1] != 3:
            if i == 1:
                os.system("cp " + metrics_file + " " + all_metric_filetmp)
            elif i > 1:
                cmd = "cut -d, -f2  " + metrics_file + "  | paste -d,  " + all_metric_filetmp + "   - > tmpfile ;  mv tmpfile " + all_metric_filetmp
                os.system(cmd)
            os.system("rm " + metrics_file)
            i += 1
        else:
            single_metric_filetmp=metrics_file

        os.system("rm " + metrics_concat_file)
        os.system("rm " + file_ascii)

    return all_metric_filetmp, single_metric_filetmp


def usage(value):
    print ("""

Usage:   neo4j_health [ optional args ]

Purpose: Report and plot various statistics about the database/cluster as well as its current state

Inputs:  
	[ -i | --interval              ]  <S|Min|H|D|W|Y> 
	[ -p | --periods               ]  <nn>
	[ -s | --startdate             ]  <yyyy-mm-dd HH:MM:SS>
	[ -e | --enddate               ]  <yyyy-mm-dd HH:MM:SS>
	[ -c | --checkdb               ]  
	[ -d | --display_precision     ]  
	[ -b | --bolt_port             ]  
	[ -v | --verbose               ]  
	[ -m | --metric                ]  <"transaction"|"page_cache"|"bolt"|"causal_clustering"|
				           "cypher"|"check_point"|"object"|"network"|"server"|
				           "jvm_gc"|"jvm_memory"|"jvm_thread"|"log_rotation"|
				           "all"|"none">

	o:  If no options are specified, metrics will be aggregated hourly over the last 24 hours of available data
	o:  Only specify startdate or enddate (not both) when also specifying interval and periods argumments
	o:  If interval and periods are both not specified, daily aggregation will be performed by default
	o:  If no metric category is specified, it will report on "ALL" metrics
	o:  To avoid running host/online-db  healthchecks, use "--checkdb=false"
	o:  To avoid running historical metric reporint, use "--metric none"
        o:  Additionally, --checkdb option will report on the following:

               Uptime
               Neo4j Kernel
               Database Restarts:
               Disk Utilization
               Neo4j Store Size
               Database/Index Size History
               Transaction Logs
               Host Processes
               vmstat
               netstats
               Neo4j Top 10 Slowest Queries
               Neo4j Top 10 Longest GC pauses
               Neo4j Node Density
               Neo4j Active Queries
               Neo4j Active Long Running Queries ( > 60s )
               Neo4j Transactions
               Neo4j Locks
               Non-default Neo4j.conf settings
               Recommended Configuration Settings
               Interesting Errors

         Examples:
               =========

               -i S -s '2019-03-29 04:42:45' -e '2019-03-29 04:43:15' -d 
	            # Report aggregation on 30 seconds between 04:42:14 and 04:43:15 hours
	
               --startdate 2019-01-01 --enddate 2019-01-11  --metric transaction -b 7617 -c
                    # Report on ten individual days, starting on 2019-01-01

               --interval D   --periods 7  --startdate 2019-01-01  --metric all
                    # Report on seven individual days, starting on 2019-01-01

               --interval D   --periods 7  --enddate 2019-01-01 --metric cypher
                    # Report on seven individual days, starting on 2008-01-01

               --interval W  --periods 4  --startdate 2019-01-01
                    # Report on four one-week periods

               --interval W  --periods 4  --enddate 2019-01-01
                    # Report on four one-week periods

               --interval Y -periods 2  --startdate 2018-01-01

Outputs:   Sample output ....

$neo4j_health -m transaction -s '2019-01-01' -e '2019-02-02' -i W

date                        2019-01-06  2019-01-13  2019-01-20  2019-01-27  2019-02-03
tx.peak_concurrent_Avg             239         239         239         239         239
tx.peak_concurrent_Max             239         239         239         239         239
tx.started-ps_Avg                  425         433         448         444         446
tx.started-ps_Max                 1651        1732        1802        1447        1376
tx.active_Avg                        8           6           7           7           8
tx.active_Max                      273         266         272         261         226
tx.active_read_Avg                   5           4           4           5           5
tx.active_read_Max                 231         233         216         220         203
tx.active_write_Avg                  3           3           3           3           4
tx.active_write_Max                239         236         245         237         226
tx.committed-ps_Avg                327         366         396         390         379
tx.committed-ps_Max               1650        1731        1800        1449        1374
tx.committed_read-ps_Avg           236         304         347         340         318
tx.committed_read-ps_Max          1650        1731        1800        1449        1374
tx.committed_write-ps_Avg           91          63          49          51          62
tx.committed_write-ps_Max          499         546         520         462         377
tx.rollbacks-ps_Avg                 99          68          53          55          67
tx.rollbacks-ps_Max                412         406         369         357         351
tx.rollbacks_read-ps_Avg            99          68          53          55          67
tx.rollbacks_read-ps_Max           412         406         369         357         351
tx.rollbacks_write-ps_Avg            1           1           1           1           1
tx.rollbacks_write-ps_Max            1           1           1          11           1
tx.terminated-ps_Avg                 1           1           1           1           1
tx.terminated-ps_Max               196         159         216         198         221
tx.terminated_read-ps_Avg            1           1           1           1           1
tx.terminated_read-ps_Max          196         159         216         198         221
tx.terminated_write-ps_Avg           0           1           1           1           1
tx.terminated_write-ps_Max




Dependencies:  
	o Export NEO4J_HOME
	o Export DB_USER
	o Export DB_PWD
	o APOC
	o Python2 or Python3
	o Pandas - sudo pip install pandas
	o ConfigObj - sudo pip install configobj
	
""")

    if value == 'interval':
        print (' ')
        print ('Example:  --interval <H|D|W|Y> ')
        print (' ')
    if value == 'metric_category':
        print ("""

        Example:  --metric  ["transaction"|"page_cache"|"bolt"|"causal_clustering"|"cypher"|"check_point"|"object"|"network"|"server"|"jvm_gc"|"jvm_memory"|"jvm_thread"|"log_rotation"|"all"]

	""")

    if value == 'dates':
        print("""

	 o:  Only specify startdate or enddate (not both) when also specifying interval and periods argumments
	 o:  If interval and periods are both not specified, daily aggregation will be performed by default

               Examples:
               =========
               --startdate 2019-01-01 --enddate 2019-01-11
                    # Report on ten individual days, starting on 2019-01-01

               --interval D   --periods 7  --startdate 2019-01-01
                    # Report on seven individual days, starting on 2019-01-01

               --interval D   --periods 7  --enddate 2019-01-01
                    # Report on seven individual days, starting on 2008-01-01

               --interval W  --periods 4  --startdate 2019-01-01
                    # Report on four one-week periods

               --interval W  --periods 4  --enddate 2019-01-01
                    # Report on four one-week periods

               --interval Y -periods 2  --startdate 2018-01-01
                    # Report on two years, starting at the beginning of year



""")


def main(argv):

    global process_metrics
    global bolt_port
    global checkdb
    global display_precision
    global verbose
    global bolt_port
    global metrics_csv_interval

    start_date = None
    end_date = None
    interval = "X"
    periods = None
    options=True
    checkdb=False
    display_precision=False
    metric_category = 'all'
    output_filename = "neo4j_health.out"

    try:
        opts, args = getopt.getopt(sys.argv[1:], 'i:b:p:s:e:m:o:cvdh', [
            'interval=', 'periods=', 'startdate=', 'enddate=', 'outfile=', 'metric=', 'port=', 'bolt_port=', 'checkdb', 'display_precision', 'verbose', 'help'])

    except getopt.GetoptError:
        usage('')
    else:
        for opt, arg in opts:
            if opt in ('-h', '--help'):
                usage(0)
                exit()
            if opt in ('-i', '-interval'):
                interval = arg
                if interval not in ("S","Min","H", "D", "W", "Y"):
                    usage(interval)
                    exit(1)
                else:
                    if interval == 'S':
                       interval_string = 'Seconds'
                    elif interval == 'Min':
                       interval_string = 'Minutes'
                    elif interval == 'H':
                       interval_string = 'Hours'
                    elif interval == 'D':
                       interval_string= 'Days'
                    elif interval == 'W':
                       interval_string= 'Weeks'
                    elif interval == 'Y':
                       interval_string= 'Years'

            elif opt in ('-p', '--periods'):
                periods = arg
            elif opt in ('-s', '--startdate'):
                start_date = arg
            elif opt in ('-e', '--enddate'):
                end_date = arg
            elif opt in ('-m', '--metric'):
                metric_category = arg
                if metric_category not in ("transaction", "page_cache", "bolt", "causal_clustering", "cypher", "check_point", "object","network", "server", "jvm_gc", "jvm_memory", "jvm_thread", "log_rotation", "all","none"):
                    print(" " )
                    print("ERROR: Incorrect parameter:" + arg)
                    print(" " )
                    usage(metric_category)
                    exit(1)
            elif opt in ('-v', '--verbose'):
                verbose = True
            elif opt in ('-c', '--checkdb'):
                checkdb=True
            elif opt in ('-d', '--display_precision'):
                display_precision=True
            elif opt in ('-b','--bolt_port'):
                bolt_port=arg
            elif opt in ('--port'):
                bolt_port = arg
            elif opt in ('-o', '--outfile'):
                output_filename = "neo_health.out"
            else:
                print("Incorrect parameter:" + arg)

        if start_date != None and end_date != None:
            if (interval != "X" and periods != None) or (interval == 'X' and periods != None) or (interval == 'X' and periods is None ) :
                usage("dates")
                exit(1)

    if len(sys.argv) == 0 or (start_date == None and end_date == None and interval == "X" and periods == None):
        interval = "D"
        interval_string = "Days"
        periods = 1
        options = False

    get_env()

    metrics_csv_interval=flag_server_restarts()

    #verbose=True

    if verbose:
      print (" ")
      print (" ")
      print ("###################################################################")
      print (" ")
      print ("Input Parameters: ")
      print (" ")
      print (" ")
      print (" ")
      print ('STARTDATE           :', start_date)
      print ('ENDDATE             :', end_date)
      print ('INTERVAL            :', interval_string)
      print ('PERIODS             :', periods)
      print ('OUTPUT              :', output_filename)
      print ('METRIC              :', metric_category)
      print ('NEO4J_HOME          :', neo4j_home)
      print ('OUTPUT-DIR          :', working_directory)
      print ('METRIC-DIR          :', metrics_directory)
      print ('BOLT-PORT           :', bolt_port)
      print ('DB-CHECK            :', checkdb)
      print ('METRICS_CSV_INTERVAL:', metrics_csv_interval)
      print (" ")
      print (" ")


    if checkdb:
      host_db_check()

    if metric_category == 'all':
        process_metrics = 'all'

        csv_filename = results_directory + "/" + all_csv_filename

        if os.path.isfile(csv_filename):
            os.system("mv " + csv_filename + " " + csv_filename + ".old")
        else:
            os.system("touch " + csv_filename )

        for metric_type in ("object","transaction", "bolt", "page_cache", "cypher", "check_point", "network", "server", "jvm_gc", "jvm_memory", "jvm_thread", "log_rotation","causal_clustering"):

            get_stats(start_date, end_date, metric_type,
                      interval, periods, options)
    else:
        get_stats(start_date, end_date, metric_category,
                  interval, periods, options)


if __name__ == "__main__":
    main(sys.argv[1:])
